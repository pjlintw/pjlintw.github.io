---
permalink: /
title: ""
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, I am a second year master's student at the [Language Science and Technology Department (LST)](https://www.uni-saarland.de/en/department/lst/research.html) of Saarland University, where I work with [Dietriech Klakow](https://www.lsv.uni-saarland.de/people/dietrich-klakow/) and [Vera Demberg](https://www.uni-saarland.de/lehrstuhl/demberg/members/verademberg.html). Prior to that, I worked with [Richard Tzong-Han Tsai](https://www.iisr.csie.ncu.edu.tw/faculty) at Intelligent Information Service Research Lab (IISR) and [Liu Yuan-ju](https://www.litphil.sinica.edu.tw/people/researchers/Liu,%20Yuan-ju) at Academia Sinica, Taiwan. 

My primary research interests broadly revolve around efficient NLP which span a wide range of captivating topics including:

- Efficient transfer learning: [In-Context Prompt Editing](https://arxiv.org/abs/2311.00895), [Open Prompt Alignment](https://arxiv.org/abs/2311.00897), [Sample Size Determination](https://aclanthology.org/2023.findings-acl.419/), [LED](https://aclanthology.org/2022.creativesumm-1.9/).
- NLP for low-resource languages: [CaT](https://arxiv.org/abs/2307.00382).
- Semantic space for task information encoding: [IIT](https://drive.google.com/file/d/1cRGYOvBls695iaOWhuV_8bJoIKy1EUMy/view?usp=sharing).

These days, I‚Äôm excited about delving into the behavior of LMs ‚Äî understanding how they learn and process information at different levels. Pertinent sub-questions arise, such as understanding the specific information transmitted through within-tuning phases and cross-tuning phases. Furthermore, exploring how LMs store information in their knowledge reservoirs and how they can selectively purge or forget knowledge from this reservoir.

Email: pinjie [at] lst.uni-saarland.de

<span style="color:darkgreen">I am on the lookout for opportunities in academia for Fall 2023 or later, with an interest in pursuing a PhD Program or securing a research position. Please take a look at my [CV](https://drive.google.com/file/d/1sibpFkI6KIGVJvma6TQE_NrjQ_ScOeIJ/view?usp=sharing).</span>

<br />

# üî• News
- *2023.09*: I have been selected for the 2023B cohort of Google's CS Research Mentorship Program (CSRMP).
- *2023.05*: üéâü•∞ Our new paper Low-Resource Cross-Lingual Adaptive Training for Nigerian Pidgin has been accepted at Interspeech 2023.
- *2023.05*: üéâüéâ Our new paper has been accepted at ACL 2023 findings. 
- *2022.10*: üéâüéâ Our new paper [Two-Stage Movie Script Summarization: An Efficient Method For Low-Resource Long Document Summarization
](https://aclanthology.org/2022.creativesumm-1.9) will be presented at the workshop on Automatic Summarization for Creative Writing at COLING 2022. Our system ranks **1st** in the Script-base track.
- *2022.04*: I will present our software project for short story recommendations (with Niyati Bafna) at Saarland university. 

- *2020.10*: I start my journey at Saarland university.

- *2019.12*: Our paper will be presented at conference DADH 2019. 
- *2019.08*: I will give a talk about Climate Event system based on Historical Meteorological Records at National Central University.
- *2019.08*: I gave a tutorial for [Hello, Sequence Labeling](https://docs.google.com/presentation/d/1jdZOhs8woyt4G0nYonhlUoFmsCGW_udfGYcsA3--Axw/edit?usp=sharing) in the Summer Program at National Central University.

- *2018.12*: Our paper will be presented at conference DADH 2018. 
- *2018.07*: I gave a invited talk about Python programming on Digital Humanities Workshop at National Taiwan University.

<br />

# üìù <a id="-Publications">Publications</a>

\* indicates equal contributions

**Low-Resource Cross-Lingual Adaptive Training for Nigerian Pidgin** <br />
**<ins>Pin-Jie Lin</ins>\***, Muhammed Saeed\*, Ernie Chang\*, Merel Scholman <br /> 
Interspeech 2023 <br />
[\[Paper\]](https://arxiv.org/abs/2307.00382) <br />

**Revisiting Sample Size Determination in Natural Language Understanding** <br />
Ernie Chang\*, Muhammad Hassan Rashid\*, **<ins>Pin-Jie Lin</ins>\***, Changsheng Zhao, Vera Demberg, Yangyang Shi and Vikas Chandra <br />
ACL 2023 Findings <br />
[\[Paper\]](https://arxiv.org/abs/2307.00374) <br />

**Two-Stage Movie Script Summarization: An Efficient Method For Low-Resource Long Document Summarization** <br />
Dongqi Pu\*, Xudong Hong\*, **<ins>Pin-Jie Lin</ins>\***, Ernie Chang, Vera Demberg <br />
COLING 2022 <br />
[\[Paper\]](https://aclanthology.org/2022.creativesumm-1.9/) <br />

**Event Extraction: Convolutional Neural Networks for Extracting Medieval
Chinese Monk‚Äôs Travels**  <br />
**<ins>Pin-Jie Lin</ins>**, Bing-Lin Hsieh  <br />
International Conference of Digital Archives and Digital Humanitie 2019 <br />

**Name Recognition of Medieval Chinese
Monk Names** <br />
Severina Balabanova, **<ins>Pin-Jie Lin</ins>**, Ya-Lin Chen, Wan-Chun Chiu <br />
International Conference of Digital Archives and Digital Humanities 2018 <br />


